{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c63cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GaussianAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        super(GaussianAttention, self).__init__(**kwargs)\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(GaussianAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.matmul(x, self.W) + self.b\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2656ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_encoder=28\n",
    "num_encoder_tokens=2048\n",
    "latent_dim=512\n",
    "time_steps_decoder=10\n",
    "num_decoder_tokens=1500\n",
    "batch_size=320\n",
    "epochs = 25\n",
    "save_model_path = './'\n",
    "import tensorflow as tf\n",
    "import graphviz\n",
    "import pydot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "encoder_inputs_global = tf.keras.layers.Input(shape=(time_steps_encoder, 2048), name=\"encoder_inputs_global\")\n",
    "encoder_inputs_motion = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_motion\")\n",
    "encoder_inputs_local = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_local\")\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Concatenate(axis=2)([encoder_inputs_global, encoder_inputs_motion, encoder_inputs_local])\n",
    "\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder_lstm1')\n",
    "encoder_seq_output, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "attention_output = GaussianAttention(return_sequences=True, name='attention')(encoder_seq_output)\n",
    "\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder_lstm2')\n",
    "_, state_h_1, state_c_1 = encoder(attention_output)\n",
    "encoder_states = [state_h_1,state_c_1]\n",
    "\n",
    "# encoder_states = [state_h]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(time_steps_decoder, num_decoder_tokens), name=\"decoder_inputs\")\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _ , _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_outputs = tf.keras.layers.Dropout(0.3)(decoder_outputs)\n",
    "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='sigmoid', name='decoder_sigmoid')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs_global,encoder_inputs_motion, encoder_inputs_local, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220d1100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3db8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
